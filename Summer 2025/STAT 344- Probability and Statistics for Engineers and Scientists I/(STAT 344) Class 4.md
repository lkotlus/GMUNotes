## Interpretations and Axioms of Probability
### Probability
- Probability is used to quantify the likelihood, or chance, that an outcome of a random experiment will occur
- The likelihood of an outcome is quantified by assigning a number from the interval $[0,1]$ to the outcome (or a percentage from 0% to 100%)
	- 0 or 0% indicates an outcome will definitely not occur
	- 1 or 100% indicates an outcome will definitely occur

 ### Subjective probability
- Degree of belief 
- Different individuals will no doubt assign different probabilities to the same outcome.
- Example:
	- What's the chance of rain today?
	- People will say different things. It's based on how much you believe something.

### Relative frequency probability
- Interpreted as the limiting value of the proportion of times the outcome occurs in $n$ repetitions of the random experiment as $n\rightarrow\infty$
- Example:
	- If we assign probability 0.2 to the outcome that there is a corrupted pulse in a digital signal, we might interpret this assignment as implying that, if we analyze many pulses, approximately 20% of them will be corrupted.

### Equally likely outcomes:
- Whenever a sample space consists of $N$ possible outcomes that are equally likely, the probability of each one is $\frac{1}{N}$

### Probability of an event:
- For a discrete sample space, the probability of an event, $E$, is denoted as $\text{P}(E)$. It is equal to the sum of the probabilities of the outcomes in $E$.
- For a continuous sample space, the probability of an event is denoted as the integral of the intervals of all outcomes. (I THINK)
- Axioms of probability:
	1. $P(S)=1$
	2. $0\le \text{P}(E)\le1$
	3. For any two events $E_1$ and $E_2$ with $E_1\cap E_2=\varnothing$, $P(E_1\cup E_2)=P(E_1)+P(E_2)$.
	- The axioms just imply that:
		- $P(\varnothing)=0$ and $P(E')=1-P(E)$
		- If $E_1$ is contained in $E_2$, then $P(E_1)\le P(E_2)$

## Unions and Events and Addition Rules
- Joint events are generated by applying basic set operations to individual events, specifically:
	- Union of events
	- Intersections of events
	- Complements of events
- Probabilities of joint events can often be determined from the probabilities of the individual events it comprises

### Addition Rule
- Probability of a union: $$P(A\cup B)=P(A)+P(B)-P(A\cap B)$$
- Mutually exclusive: $$P(E_1\cup E_2\cup ...\cup E_k)=\sum_{i=0}^kP(E_i)$$

## Conditional Probability
- The probability of an event, $B$, under the knowledge that the outcome will be in event $A$ is called the conditional probability of $B$ given $A$, denoted as $P(B|A)$.
- A digital communications channel has an error rate of 1 per 1000 bits transmitted. Errors are rare and occur in bursts. If a single bit is transmitted, we might model the probability of an error as 1 in 1000. However, if the previous bit was an error because of the bursts, we might believe that the probability that the next bit will be an error is greater than 1 in 1000.

### Conditional Probability
- The conditional probability of an event, $B$, given an event, $A$, is calculated with (given $P(A)>0$): $$P(B|A)=\frac{P(A\cap B)}{P(A)}$$

### Random Samples
- To select randomly implies that at each step of the sample, the items that remain in the batch are equally likely to be selected
	- We cannot obtain a random sample by haphazardly picking a sample on our own.
	- It is known that individuals cannot select a proper random sample without using an objective randomization method (such as technology)

### Multiplication Rule
-  Multiplication rule for probabilities: $$P(A\cap B)=P(B|A)\cdot P(A)=P(A|B)\cdot P(B)$$
- This statement is interchangeable for $A$ and $B$.

### Total Probability Rule
- $A$ and $A'$ are mutually exclusive
- $A\cap B$ and $A'\cap B$ are mutually exclusive
- $B=(A\cap B)\cup(A'\cap B)$
- Total probability rule:
	- For any two events $A$ and $B$: $$P(B)=P(B\cap A)+P(B\cap A')=P(B|A)\cdot P(A)+P(A|B)\cdot P(B)$$
- Extended: 
	- If $E_1, E_2, ..., E_k$ are mutually exclusive, then: $$P(B)=\sum_{i=1}^k P(B\cap E_i)=\sum_{i=1}^k(P(B|E_i)\cdot E_i)$$

### Independence
- Two events are independent if any one of the following equivalent statements is true:
	1. $P(A|B)=P(A)$
	2. $P(B|A)=P(B)$
	3. $P(A\cap B)=P(A)\cdot P(B)$
- Essentially just knowledge that the outcome of the experiment is in event $A$ does not affect the probability that the outcome is in event $B$.
- Multiple events:
	- We are independent if: $$P(E_1\cap E_2\cap ...\cap E_k)=\prod_{i=1}^kP(E_i)$$

### Bayes' Theorem
- Thomas Bayes addressed essential questions in the 1700s of finding the probability that a condition was present (high contamination) given an outcome (a semiconductor failure).
- Bayes' theorem states that for $P(B)>0$: $$P(A|B)=\frac{P(B|A)\cdot P(A)}{P(B)}$$
- This can just be derived from the multiplication rule.
- For $k$ events: $$P(E_1|B)=\frac{P(B|E_1)\cdot P(E_1)}{\sum_{i=1}^k(P(B|E_i)\cdot P(E_i))}$$

### Random Variables
- A random variable is a function that assigns a real number to each outcome in the sample space of a random experiment
- Notation is used to distinguish between a random variable and the real number:
	- Random variables are denoted by uppercase
		- Length: $L$
	- Measured values are denoted by lowercase
		- Length: $l$
- Discrete:
	- A random variable with finite (or countably infinite) range
		- Number of scratches on a surface, proportion of parts tested among 1000 tested, ...
	- A random variable with an interval of real numbers for its range
		- Current, length, pressure, ...

## Important Terms (Classes 3 and 4)
- Addition Rule
- Axioms of probability
- Bayes' theorem
- Combination
- Conditional probability
- counting techniques
- Equally likely outcomes
- Events
- Independence
- Multiplication rule
- Mutually exclusive events
- outcome
- Permutation
- Probability
- Random samples
- Random variables (discrete and continuous)
- Sample spaces (discrete and continuous)
- Total probability rule
- Tree diagrams
- Venn diagrams
- With or without replacement